import pandas as pd
import pyspark
from datetime import datetime
from pyspark.sql.types import *
import graphene
import flask
import flask_graphql
import deltaGQL
import os
import gc
from flask import Flask
from flask_graphql import GraphQLView

# Check if spark session is already running or create new one                                                                                                                                               
from pyspark.sql.session import SparkSession
spark = SparkSession.builder.getOrCreate()

# Load latest data:                                                                                                                                                                                         
deltapath = os.getenv("RUBIXTAPEDELTAPATH")

# Load monthly dfs:                                                                                                                                             
monthly_GQL_PO_AW = deltaGQL.load_latest_version(deltapath+"/GQL_PO_AW")
monthly_GQL_PO_APPR_HDR_VW = deltaGQL.load_latest_version(deltapath+"/GQL_PO_APPR_HDR_VW")
monthly_GQL_EOAW_STEPINST = deltaGQL.load_latest_version(deltapath+"/GQL_EOAW_STEPINST")
monthly_GQL_EOAW_USERINST = deltaGQL.load_latest_version(deltapath+"/GQL_EOAW_USERINST")
monthly_GQL_PSOPRDEFN = deltaGQL.load_latest_version(deltapath+"/GQL_PSOPRDEFN")
print("Loaded monthly tables")

# Load daily dfs:
daily_GQL_PO_AW = deltaGQL.load_latest_version(deltapath+"/GQL_PO_AW-daily_pulls")
daily_GQL_PO_APPR_HDR_VW = deltaGQL.load_latest_version(deltapath+"/GQL_PO_APPR_HDR_VW-daily_pulls")
daily_GQL_EOAW_STEPINST = deltaGQL.load_latest_version(deltapath+"/GQL_EOAW_STEPINST-daily_pulls")
daily_GQL_EOAW_USERINST = deltaGQL.load_latest_version(deltapath+"/GQL_EOAW_USERINST-daily_pulls")
daily_GQL_PSOPRDEFN = deltaGQL.load_latest_version(deltapath+"/GQL_PSOPRDEFN-daily_pulls")
print("Loaded daily tables")

# Load pks:
table_pkeydictionary = deltaGQL.primary_keys
pk_GQL_PO_AW = deltaGQL.find_keys(table_pkeydictionary, "GQL_PO_AW")
pk_GQL_PO_APPR_HDR_VW = deltaGQL.find_keys(table_pkeydictionary, "GQL_PO_APPR_HDR_VW")
pk_GQL_EOAW_STEPINST = deltaGQL.find_keys(table_pkeydictionary, "GQL_EOAW_STEPINST")
pk_GQL_EOAW_USERINST = deltaGQL.find_keys(table_pkeydictionary, "GQL_EOAW_USERINST")
pk_GQL_PSOPRDEFN = deltaGQL.find_keys(table_pkeydictionary, "GQL_PSOPRDEFN")
print("Found primary keys")

# Upsert data to get latest data:
GQL_PO_AW = deltaGQL.upsert_spark(monthly_GQL_PO_AW , daily_GQL_PO_AW , pk_GQL_PO_AW)
GQL_PO_APPR_HDR_VW = deltaGQL.upsert_spark(monthly_GQL_PO_APPR_HDR_VW , daily_GQL_PO_APPR_HDR_VW , pk_GQL_PO_APPR_HDR_VW)
GQL_EOAW_STEPINST = deltaGQL.upsert_spark(monthly_GQL_EOAW_STEPINST , daily_GQL_EOAW_STEPINST , pk_GQL_EOAW_STEPINST)
GQL_EOAW_USERINST = deltaGQL.upsert_spark(monthly_GQL_EOAW_USERINST , daily_GQL_EOAW_USERINST , pk_GQL_EOAW_USERINST)
GQL_PSOPRDEFN = deltaGQL.upsert_spark(monthly_GQL_PSOPRDEFN , daily_GQL_PSOPRDEFN , pk_GQL_PSOPRDEFN)
print("Merged monthly and daily tables")

del [[monthly_GQL_PO_AW, monthly_GQL_PO_APPR_HDR_VW, monthly_GQL_EOAW_STEPINST, monthly_GQL_EOAW_USERINST, monthly_GQL_PSOPRDEFN, daily_GQL_PO_AW, daily_GQL_PO_APPR_HDR_VW, daily_GQL_EOAW_STEPINST, daily_GQL_EOAW_USERINST, daily_GQL_PSOPRDEFN]]
gc.collect()
print("Running Query")

alldfs = [var for var in dir() if isinstance(eval(var), pyspark.sql.dataframe.DataFrame)]

num = []
name = []
for df in alldfs:
    num.append(eval(df).count())
    name.append(df)
print(zip(name, num))


# Create views:                                                                                                                                                                                             
views = []
for df in alldfs:
    eval(df).createOrReplaceTempView(df)
    views.append(df)
print(views)

# Run SQL:
spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")
sqlDF = ('''SELECT DISTINCT A.BUSINESS_UNIT, A.PO_ID, date_format(B.PO_DT, 'MM/dd/yyyy') AS PO_DT, B.PO_STATUS, D.OPRID, K.OPRDEFNDESC, from_unixtime(unix_timestamp(D.DTTM_CREATED, "yyyy-MM-dd HH:mm:ss"),'MM/dd/yyyy hh:mm:ss aa') AS FORMATTED_DTTM_CREATED, from_unixtime(unix_timestamp(D.EOAWDTTM_MODIFIED, "yyyy-MM-dd HH:mm:ss"),'MM/dd/yyyy hh:mm:ss aa') AS FORMATTED_EOAWDTTM_MODIFIED, (D.EOAWDTTM_MODIFIED - D.DTTM_CREATED) AS PENDING_TIME, D.EOAWORIG_OPRID, D.EOAWSTEP_STATUS, D.EOAWSTEP_INSTANCE
  FROM GQL_PO_AW A, GQL_PO_APPR_HDR_VW B, GQL_EOAW_STEPINST C, GQL_EOAW_USERINST D, GQL_PSOPRDEFN K
  WHERE ( B.BUSINESS_UNIT = A.BUSINESS_UNIT
     AND B.PO_ID = A.PO_ID
     AND A.EOAWTHREAD_ID = C.EOAWTHREAD_ID
     AND A.EOAWPRCS_ID = C.EOAWPRCS_ID
     AND C.EOAWDEFN_ID = A.EOAWDEFN_ID
     AND C.EOAWSTEP_INSTANCE = D.EOAWSTEP_INSTANCE
     AND A.EOAWTHREAD_ID = (SELECT MIN( E.EOAWTHREAD_ID)
  FROM GQL_PO_AW E
  WHERE E.BUSINESS_UNIT = A.BUSINESS_UNIT
     AND E.PO_ID = A.PO_ID
     AND E.LINE_NBR = A.LINE_NBR
     AND E.EOAWTHREAD_STATUS <> 'T')
     AND C.EOAWTHREAD_ID = (SELECT MIN(  F.EOAWTHREAD_ID)
  FROM GQL_PO_AW F, GQL_PO_APPR_HDR_VW G, GQL_EOAW_STEPINST H, GQL_EOAW_USERINST I
  WHERE G.BUSINESS_UNIT = A.BUSINESS_UNIT
     AND G.PO_ID = A.PO_ID
     AND G.BUSINESS_UNIT = F.BUSINESS_UNIT
     AND G.PO_ID = F.PO_ID
     AND G.HOLD_STATUS = 'N'
     AND F.EOAWTHREAD_ID = H.EOAWTHREAD_ID
     AND F.EOAWPRCS_ID = H.EOAWPRCS_ID
     AND H.EOAWDEFN_ID = F.EOAWDEFN_ID
     AND H.EOAWSTEP_INSTANCE = I.EOAWSTEP_INSTANCE
     AND F.EOAWTHREAD_ID = (SELECT MIN( J.EOAWTHREAD_ID)
  FROM GQL_PO_AW J
  WHERE J.BUSINESS_UNIT = F.BUSINESS_UNIT
     AND J.PO_ID = F.PO_ID
     AND J.LINE_NBR = F.LINE_NBR
     AND J.EOAWTHREAD_STATUS <> 'T'))
     AND K.OPRID = D.OPRID
     AND A.PO_ID IN ('9000009345','9000008849','9000008896')
     AND A.PO_ID = '9000008896')
  ORDER BY 2, 8, 12''')

'''sqlDF.select("BUSINESS_UNIT", "PO_ID", "PO_DT", "PO_STATUS", "OPRID", "OPRDEFNDESC", "FORMATTED_DTTM_CREATED", "FORMATTED_EOAWDTTM_MODIFIED", "PENDING_TIME", "EOAWORIG_OPRID", "EOAWSTEP_STATUS", "EOAWSTEP_INSTANCE").show()'''


sqlDF = spark.sql('''SELECT A.BUSINESS_UNIT, A.REQ_ID, A.REQ_STATUS, A.HOLD_STATUS, TO_CHAR(A.REQ_DT,'YYYY-MM-DD'), A.ORIGIN, A.OPRID_ENTERED_BY, A.OPRID_MODIFIED_BY, TO_CHAR(A.ENTERED_DT,'YYYY-MM-DD'), TO_CHAR(A.APPROVAL_DT,'YYYY-MM-DD'), B.MB_HOLD_STATUS, B.BUYER_ID, B.LAST_UPDATED_BY, TO_CHAR(CAST((B.LAST_UPDATE_DTTM) AS TIMESTAMP),'YYYY-MM-DD-HH24.MI.SS.FF'), C.LINE_NBR, TO_CHAR(C.DUE_DT,'YYYY-MM-DD'), C.MB_HOLD_TYPE, C.COMMENTS60, F.BUSINESS_UNIT, F.PO_ID, F.LINE_NBR, TO_CHAR(SYSDATE,'YYYY-MM-DD'), TO_CHAR(SYSDATE,'YYYY-MM-DD'), TO_CHAR(SYSDATE,'YYYY-MM-DD')
  FROM PS_REQ_HDR A, PS_MB_REQ_HOLD_HDR B, PS_MB_REQ_HOLD_LN C, PS_REQ_LINE D, (PS_REQ_LN_DISTRIB E LEFT OUTER JOIN  PS_PO_LINE_DISTRIB F ON  E.BUSINESS_UNIT = F.BUSINESS_UNIT AND E.REQ_ID = F.REQ_ID AND E.LINE_NBR = F.LINE_NBR AND E.SCHED_NBR = F.SCHED_NBR AND E.DISTRIB_LINE_NUM = F.DISTRIB_LINE_NUM )
  WHERE ( A.BUSINESS_UNIT = B.BUSINESS_UNIT
     AND A.REQ_ID = B.REQ_ID
     AND B.BUSINESS_UNIT = C.BUSINESS_UNIT
     AND B.REQ_ID = C.REQ_ID
     AND A.BUSINESS_UNIT = D.BUSINESS_UNIT
     AND A.REQ_ID = D.REQ_ID
     AND D.BUSINESS_UNIT = E.BUSINESS_UNIT
     AND D.REQ_ID = E.REQ_ID
     AND D.LINE_NBR = E.LINE_NBR
     AND C.MB_LINE_STATUS IN ('C','N','X'))
  ORDER BY 4 DESC, 5 DESC, 2, 15''')




sqlDF.persist()
print("Data loaded in cache")

print("Total records: {}".format(sqlDF.count()))
sqlDF.select("PO_DT").show()